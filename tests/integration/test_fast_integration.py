#!/usr/bin/env python3
"""
Fast Integration Test for Data Processing Pipeline

This test validates the end-to-end pipeline works correctly with a small
subset of documents from R1202009, then tests that the full system can
load and query the data properly.

Author: Claude Code
"""

import unittest
import subprocess
import json
import logging
import shutil
from pathlib import Path
from datetime import datetime

# Setup test logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TestFastIntegration(unittest.TestCase):
    """Fast integration test with subset of documents"""
    
    @classmethod
    def setUpClass(cls):
        """Setup test environment"""
        cls.test_proceeding = 'R1202009'
        cls.max_processing_time = 300  # 5 minutes max
        
        print(f"\nüß™ Fast Integration Test Setup")
        print(f"üìã Test Proceeding: {cls.test_proceeding}")
        print("=" * 60)
    
    def test_01_prerequisites(self):
        """Test that prerequisites are in place"""
        print(f"\nüìã Step 1: Validating Prerequisites")
        
        # Check proceeding directory exists
        proceeding_dir = Path('cpuc_proceedings') / self.test_proceeding
        self.assertTrue(proceeding_dir.exists(), 
                       f"Proceeding directory should exist: {proceeding_dir}")
        
        # Check scraped PDF history exists
        import config
        paths = config.get_proceeding_file_paths(self.test_proceeding)
        
        scraped_history_exists = (paths['scraped_pdf_history'].exists() or 
                                paths['scraped_pdf_history_alt'].exists())
        self.assertTrue(scraped_history_exists, "Scraped PDF history should exist")
        
        # Load scraped data
        from standalone_data_processor import load_scraped_pdf_data
        scraped_data = load_scraped_pdf_data(self.test_proceeding)
        self.assertIsNotNone(scraped_data, "Should be able to load scraped data")
        self.assertGreater(len(scraped_data), 0, "Should have scraped PDFs")
        
        print(f"   ‚úÖ Prerequisites validated: {len(scraped_data)} PDFs available")

    def test_02_subset_processing(self):
        """Test processing with a small subset of documents"""
        print(f"\nüîÑ Step 2: Processing Document Subset")
        
        import config
        paths = config.get_proceeding_file_paths(self.test_proceeding)
        
        # Clean up any existing processed data
        if paths['vector_db'].exists():
            shutil.rmtree(paths['vector_db'])
            print(f"   üßπ Cleaned existing vector DB")
        
        if paths['embeddings_dir'].exists():
            shutil.rmtree(paths['embeddings_dir'])
            print(f"   üßπ Cleaned existing embeddings")
        
        # Create a subset of documents for testing
        from standalone_data_processor import load_scraped_pdf_data
        scraped_data = load_scraped_pdf_data(self.test_proceeding)
        
        # Take only first 3 documents for fast testing
        subset_data = dict(list(scraped_data.items())[:3])
        
        # Create temporary scraped history file with subset
        temp_history_file = paths['proceeding_dir'] / 'temp_scraped_pdf_history.json'
        with open(temp_history_file, 'w') as f:
            json.dump(subset_data, f, indent=2)
        
        print(f"   üìÑ Created subset with {len(subset_data)} documents")
        
        try:
            # Update config to point to temp file temporarily
            original_path = paths['scraped_pdf_history']
            alt_path = paths['scraped_pdf_history_alt']
            
            # Rename temp file to expected location
            if original_path.exists():
                original_path.rename(original_path.parent / 'backup_scraped_pdf_history.json')
            temp_history_file.rename(original_path)
            
            # Run data processor on subset
            print(f"   üöÄ Processing {len(subset_data)} documents...")
            
            start_time = datetime.now()
            result = subprocess.run([
                'python', 'standalone_data_processor.py', self.test_proceeding,
                '--batch-size', '2',
                '--verbose'
            ], 
            capture_output=True, 
            text=True, 
            timeout=self.max_processing_time,
            cwd=Path.cwd()
            )
            
            processing_time = datetime.now() - start_time
            print(f"   ‚è±Ô∏è Processing completed in {processing_time}")
            
            if result.returncode != 0:
                print(f"   ‚ùå Processing stderr: {result.stderr}")
                print(f"   üìù Processing stdout: {result.stdout}")
                self.fail(f"Subset processing failed with return code {result.returncode}")
            
            print(f"   ‚úÖ Subset processing successful")
            
        finally:
            # Restore original files
            if original_path.exists():
                original_path.unlink()
            backup_file = original_path.parent / 'backup_scraped_pdf_history.json'
            if backup_file.exists():
                backup_file.rename(original_path)
            if temp_history_file.exists():
                temp_history_file.unlink()

    def test_03_validate_vector_store(self):
        """Test that vector store was created properly"""
        print(f"\nüóÑÔ∏è Step 3: Validate Vector Store")
        
        import config
        paths = config.get_proceeding_file_paths(self.test_proceeding)
        
        # Check vector DB directory exists
        self.assertTrue(paths['vector_db'].exists(), 
                       f"Vector DB directory should exist: {paths['vector_db']}")
        
        # Check for LanceDB files
        lance_files = list(paths['vector_db'].rglob("*"))
        self.assertGreater(len(lance_files), 0, "Should have LanceDB files")
        print(f"   ‚úÖ LanceDB files: {len(lance_files)} files created")
        
        # Check document hashes
        if paths['document_hashes'].exists():
            with open(paths['document_hashes'], 'r') as f:
                doc_hashes = json.load(f)
            print(f"   ‚úÖ Document hashes: {len(doc_hashes)} documents processed")
        else:
            print(f"   ‚ö†Ô∏è No document hashes file found")

    def test_04_rag_system_functionality(self):
        """Test RAG system can load and query the processed data"""
        print(f"\nü§ñ Step 4: RAG System Functionality")
        
        # Initialize RAG system
        from rag_core import CPUCRAGSystem
        
        rag_system = CPUCRAGSystem(current_proceeding=self.test_proceeding)
        self.assertIsNotNone(rag_system, "RAG system should initialize")
        
        # Check if vector store loaded
        if rag_system.vectordb is not None:
            print(f"   ‚úÖ Vector store loaded")
            
            # Get system stats
            stats = rag_system.get_system_stats()
            chunk_count = stats.get('total_chunks', 0)
            print(f"   ‚úÖ System stats: {chunk_count} chunks loaded")
            
            # Test basic query if we have chunks
            if chunk_count > 0:
                print(f"   üîç Testing query functionality...")
                test_query = "What documents are available?"
                
                try:
                    query_results = []
                    for result in rag_system.query(test_query):
                        if isinstance(result, dict):
                            query_results.append(result)
                            break
                    
                    if query_results:
                        final_result = query_results[0]
                        self.assertIn('answer', final_result, "Should have answer field")
                        self.assertIn('sources', final_result, "Should have sources field")
                        print(f"   ‚úÖ Query successful: {len(final_result['sources'])} sources")
                    else:
                        print(f"   ‚ö†Ô∏è Query completed but no final results")
                        
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Query failed: {e}")
                    # Don't fail the test for query issues
            else:
                print(f"   ‚ö†Ô∏è No chunks available for querying")
        else:
            print(f"   ‚ö†Ô∏è Vector store not loaded")

    def test_05_app_integration(self):
        """Test that the app can detect and load the processed data"""
        print(f"\nüåê Step 5: App Integration")
        
        # Test proceeding detection
        from app import get_available_proceedings_from_db
        
        available_proceedings = get_available_proceedings_from_db()
        
        if self.test_proceeding in available_proceedings:
            print(f"   ‚úÖ Proceeding detected in app: {available_proceedings}")
        else:
            print(f"   ‚ö†Ô∏è Proceeding not detected in app: {available_proceedings}")
            # Don't fail for this since the app might need full data
        
        # Test basic config integration 
        import config
        paths = config.get_proceeding_file_paths(self.test_proceeding)
        print(f"   ‚úÖ Config paths validated")

    def generate_test_summary(self):
        """Generate test summary"""
        print(f"\nüìä FAST INTEGRATION TEST SUMMARY")
        print("=" * 60)
        print(f"üß™ Test Proceeding: {self.test_proceeding}")
        print(f"üìÖ Test Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            import config
            paths = config.get_proceeding_file_paths(self.test_proceeding)
            
            # Show what was created
            if paths['vector_db'].exists():
                db_files = list(paths['vector_db'].rglob("*"))
                print(f"üóÑÔ∏è Vector DB Files: {len(db_files)}")
            
            if paths['document_hashes'].exists():
                with open(paths['document_hashes'], 'r') as f:
                    doc_hashes = json.load(f)
                print(f"üìö Documents Processed: {len(doc_hashes)}")
            
            # Test RAG system one more time
            from rag_core import CPUCRAGSystem
            rag_system = CPUCRAGSystem(current_proceeding=self.test_proceeding)
            if rag_system.vectordb:
                stats = rag_system.get_system_stats()  
                print(f"üìä Total Chunks: {stats.get('total_chunks', 0)}")
                print(f"üîã Vector Store: {stats.get('vector_store_status', 'unknown')}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error generating summary: {e}")
        
        print("=" * 60)
        print("‚úÖ FAST INTEGRATION TEST COMPLETED!")


def run_fast_test():
    """Run the fast integration test"""
    suite = unittest.TestLoader().loadTestsFromTestCase(TestFastIntegration)
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)
    
    # Generate summary
    if result.wasSuccessful():
        test_instance = TestFastIntegration()
        test_instance.generate_test_summary()
    
    return result.wasSuccessful()


if __name__ == '__main__':
    print(f"üöÄ Starting Fast Integration Test")
    print(f"üìã This test processes a small subset of R1202009 documents")
    print(f"‚è±Ô∏è Expected duration: 3-5 minutes")
    print("=" * 80)
    
    success = run_fast_test()
    exit_code = 0 if success else 1
    
    print(f"\nüéØ Test Result: {'PASSED' if success else 'FAILED'}")
    exit(exit_code)