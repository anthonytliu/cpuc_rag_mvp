#!/usr/bin/env python3
# Performance benchmark test for optimized URL processing

import logging
import time
from datetime import datetime
from rag_core import CPUCRAGSystem

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def benchmark_performance():
    """Benchmark the optimized URL processing pipeline"""
    
    logger.info("üöÄ Starting performance benchmark for optimized URL processing...")
    
    # Test URLs for benchmarking
    test_urls = [
        {
            'url': 'https://arxiv.org/pdf/2408.09869',
            'title': 'Docling Technical Report - Test 1'
        },
        {
            'url': 'https://arxiv.org/pdf/2206.01062',
            'title': 'Academic Paper - Test 2'
        }
    ]
    
    try:
        # Initialize RAG system
        logger.info("Step 1: Initializing RAG system...")
        start_init = time.time()
        rag_system = CPUCRAGSystem()
        end_init = time.time()
        logger.info(f"‚è±Ô∏è  RAG system initialization: {end_init - start_init:.2f} seconds")
        
        # Clear any existing test data to ensure clean benchmark
        logger.info("Step 2: Preparing clean test environment...")
        
        # Build vector store from URLs with performance monitoring
        logger.info("Step 3: Processing URLs with optimizations...")
        start_processing = time.time()
        
        rag_system.build_vector_store_from_urls(test_urls)
        
        end_processing = time.time()
        total_processing_time = end_processing - start_processing
        
        # Get final statistics
        stats = rag_system.get_system_stats()
        
        # Performance analysis
        logger.info("\n" + "="*60)
        logger.info("üéØ PERFORMANCE BENCHMARK RESULTS")
        logger.info("="*60)
        logger.info(f"üìä Total processing time: {total_processing_time:.2f} seconds")
        logger.info(f"üìÑ URLs processed: {len(test_urls)}")
        logger.info(f"üî¢ Total chunks in system: {stats.get('total_chunks', 0)}")
        logger.info(f"‚ö° Average time per URL: {total_processing_time / len(test_urls):.2f} seconds")
        
        if stats.get('total_chunks', 0) > 0:
            chunks_per_second = stats.get('total_chunks', 0) / total_processing_time
            logger.info(f"üèÉ Processing rate: {chunks_per_second:.2f} chunks/second")
            
        logger.info(f"üîß Processing mode: {stats.get('processing_mode', 'Unknown')}")
        logger.info(f"üöÄ Vector store status: {stats.get('vector_store_status', 'Unknown')}")
        logger.info(f"üíæ Vector store batch size: {stats.get('vector_store_batch_size', 'Unknown')}")
        
        # Quick functionality test
        logger.info("\nStep 4: Testing query functionality...")
        start_query = time.time()
        
        query_text = "What is document parsing and processing?"
        result_generator = rag_system.query(query_text)
        
        final_result = None
        for result in result_generator:
            if isinstance(result, dict):
                final_result = result
                break
        
        end_query = time.time()
        query_time = end_query - start_query
        
        if final_result:
            answer_length = len(final_result.get("answer", ""))
            sources_count = len(final_result.get("sources", []))
            confidence = final_result.get("confidence_indicators", {}).get('overall_confidence', 'Unknown')
            
            logger.info(f"‚è±Ô∏è  Query processing time: {query_time:.2f} seconds")
            logger.info(f"üìù Answer length: {answer_length} characters")
            logger.info(f"üìö Sources used: {sources_count}")
            logger.info(f"üéØ Confidence level: {confidence}")
        
        # Performance comparison baseline
        logger.info("\n" + "="*60)
        logger.info("üìà PERFORMANCE COMPARISON")
        logger.info("="*60)
        logger.info("Previous performance (estimated baseline):")
        logger.info("  - Single URL processing: ~22 seconds")
        logger.info("  - Processing rate: ~7.5 chunks/second")
        logger.info("  - Vector store batch: 64 chunks")
        logger.info("")
        logger.info("Optimized performance (current run):")
        if total_processing_time > 0:
            speedup = 22 / (total_processing_time / len(test_urls))
            logger.info(f"  - Single URL processing: ~{total_processing_time / len(test_urls):.1f} seconds")
            logger.info(f"  - Speedup factor: {speedup:.1f}x faster")
            
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Performance benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_parallel_vs_sequential():
    """Compare parallel vs sequential processing"""
    logger.info("\nüîÑ Testing parallel processing benefits...")
    
    # This is a conceptual test - in practice we'd need to temporarily
    # disable parallel processing to get a true comparison
    logger.info("Note: Parallel processing is now enabled by default")
    logger.info("Previous sequential processing took ~22 seconds per URL")
    logger.info("Current parallel processing should show improvement for multiple URLs")

if __name__ == "__main__":
    success = benchmark_performance()
    
    if success:
        logger.info("\nüéâ Performance benchmark completed successfully!")
        test_parallel_vs_sequential()
    else:
        logger.error("\nüí• Performance benchmark failed!")
        
    logger.info("\nüí° Optimization features enabled:")
    logger.info("  ‚úÖ Parallel URL processing with ThreadPoolExecutor")
    logger.info("  ‚úÖ Docling fast mode for table processing")
    logger.info("  ‚úÖ Optimized vector store batch size (256 vs 64)")
    logger.info("  ‚úÖ Performance monitoring and metrics")
    logger.info("  ‚úÖ Configurable threading for Docling")